---
title: "R Notebook - Confidence Intervals"
output: html_notebook
---

# Confidence Intervals

Confidence intervals use data collected from a sample to estimate a population parameter (statistics inference).

A range computed using sample statistics to estimate an unknown population parameter with a stated level of confidence.

Used in :

-   Cases where we don't have access to the whole population, a confidence level can be built from the sample.

-   To estimate the population distribution assuming a **stated** level of confidence.

### Relevant Statistic Parameters

|                             | Population Parameter | Sample Statistic |
|-----------------------------|----------------------|------------------|
| Mean                        | µ                    | x ¯              |
| Difference in 2 means       | µ1 − µ2              | x1¯ − x2¯        |
| Proportion                  | p                    | p\^              |
| Difference in 2 proportions | p1 − p2              | p1 \^ - p2\^     |
| Correlation                 | ρ                    | r                |

## Sampling Distributions

Sample statistics are random variables because they vary from sample to sample (random sampling).

As a result, sample statistics have a distribution called the **sampling distribution.**

#### Remark

-   For a single categorical variable this may be referred to as the standard error of the proportion.

-   For a single quantitative variable this may be referred to as the standard error of the mean.

-   If a sampling distribution is constructed using data from a population, the mean of the sampling distribution will be approximately equal to the population parameter.

Thus

#### Sampling Distribution

Distribution of sample statistics with a mean approximately equal to the mean in the original distribution and a standard deviation known as the standard error

#### Standard Error

Standard deviation of a sampling distribution

### Using R to Construct a Sampling Distribution Given a Known Population Proportion

Note that this method of constructing a sampling distribution requires that we have population data, where in most cases we do not know all of the population values.

(If we did, then we wouldn't need to construct a confidence interval to estimate the population parameter!)

As you look through the following examples, note that when the sample size is large the sampling distribution is approximately symmetrical and centered at the population parameter.

#### Constructing a Sampling distribution from a known population

-   The process of constructing a sampling distribution from a known population is the same for all types of parameters (i.e.,one group proportion, one group mean, difference in two proportions, difference in two means and correlation).

    1.  take a simple random sample of n from the population without replacement

    2.  record the sample statistic of interest

    3.  return those observations back into the population

    4.  repeat many times

#### Remark

-   If the sample size is large, the sampling distribution will be approximately normally distributed with a mean equal to the population parameter

**That distribution of sample statistics is known as the sampling distribution.**

##### Example in R for a Sampling distribution for a quantitative variable

```{r}
# Constructing a sampling distribution using the “NFL Contracts (2015 in Millions)” dataset that is built into the sampling distribution for a mean feature in R.

# This dataset includes the salaries of all 2,099 NFL players in 2015 as of the start of that season. We’ll construct a sampling distribution given n = 5.

#load the dataser
nfl=read.csv("../datasets/NFL2015.csv", header=T)
head(nfl)

# The five number summary of the yearly salaries
summary(nfl$YearlySalary)

# how may observation do we have?
dim(nfl)

# let's look at the histogram
hist(nfl$YearlySalary)

# Defining sampling size and creating an object to store results
n=5 # sample size
# object to store samples 
s<-matrix(NA, nrow=5, ncol=5000) 

#First sample: there are a total of 2099 cases; sampling 5 case numbers between 1 and 2099 and storing the data
s[,1]=nfl$YearlySalary[sample(1:2099,n)] 
mean(s[,1]) # and its mean 
sd(s[,1]) # and its se
median(s[,1]) # its median 

#Second sample: there are a total of 2099 cases; sampling 5 case numbers between 1 and 2099 and storing the data
s[,2]=nfl$YearlySalary[sample(1:2099,n)]
mean(s[,2]) # and its mean 
sd(s[,2]) # and its se
median(s[,2]) # its median

#First 1000 samples: there are a total of 2099 cases; sampling 5 case numbers between 1 and 2099 and storing the data; plotting the histogram os the sample means

for (i in 3:1000){
  s[,i]=nfl$YearlySalary[sample(1:2099,n)]
}
mean.s=colMeans(s[,1:1000]) 
hist(mean.s, 
     main="Histogram of 1000 sample means",
     xlab=substitute(paste("Mean=", 
                           mean, "; Stdev=",
                           std, " of 1000 samples"),
                     list(mean=signif(mean(mean.s),3), 
                          std=signif(sd(mean.s),3))))


```

```{r}
# First 2000 samples: there are a total of 2099 cases; sampling 5 case numbers between 1 and 2099 and storing the data; plotting the histogram os the sample means

for (i in 1001:2000){
  s[,i]=nfl$YearlySalary[sample(1:2099,n)]
}
mean.s=colMeans(s[,1:2000]) 
hist(mean.s, main="Histogram of 2000 sample means", 
     xlab=substitute(paste("Mean=", 
                           mean, "; Stdev=",
                           std, " of 2000 samples"), 
                     list(mean=signif(mean(mean.s),3),
                          std=signif(sd(mean.s),3))))



```

```{r}
# Last 3000 samples: there are a total of 2099 cases; sampling 5 case numbers between 1 and 2099 and storing the data; plotting the histogram of the sample means

for (i in 2001:5000){
  s[,i]=nfl$YearlySalary[sample(1:2099,n)]
}
mean.s=colMeans(s) # additionally 1000 sample means 
hist(mean.s, main="Histogram of 5000 sample means",
     xlab=substitute(paste("Mean=", 
                           mean, "; Stdev=", 
                           std, " of 5000 samples"),
                     list(mean=signif(mean(mean.s),3),
                          std=signif(sd(mean.s),3))))

```

##### **Example in R for a Categorical variable sampling distribution**

We are conducting an experiment in which we are flipping a fair coin 5 times and counting how many times we flip heads.

Whether or not the coin lands on heads is a categorical variable with a probability of 0.50.

Using R to construct a distribution of sample proportions that we could use to determine the probability of any of the possible combinations of successes and failures.

```{r}
p=0.5 # probability of success (heads)
n=5 # sample size
# object to store samples 
s<-matrix(NA, nrow=5, ncol=10000) 
# first 1000 samples
for (i in 1:1000){
  s[,i]=rbinom(n,1,p)
}

# respective 1000 sample means
mean.s=colMeans(s[,1:1000])
# frequency plot
plot(table(mean.s), 
     main="Histogram of 1000 sample means", 
     ylab="Frequency", 
     xlab=substitute(paste("Mean=", 
                           mean, "; Stdev=",
                           std, " of 1000 samples"),
                     list(mean=signif(mean(mean.s),3),
                          std=signif(sd(mean.s),3))))


```

```{r}
# additional 7000 samples 
for (i in 1001:8000){
  #randomizing results one by one with a probability of 0,5 (p)
  s[,i]=rbinom(n,1,p)
}
# calculating the mean of the means of all columns
mean.s=colMeans(s[,1:8000])
#ploting the histogram
plot(table(mean.s), 
     main="Histogram of 8000 sample means",
     ylab="Frequency", 
     xlab=substitute(paste("Mean=", 
                           mean, "; Stdev=",
                           std, " of 8000 samples"),
                     list(mean=signif(mean(mean.s),3),
                          std=signif(sd(mean.s),3))))



```

### Impact of Sample Size

There is an inverse relationship between sample size and standard error.

In other words:

-   As the sample size increases, the variability of sampling distribution decreases.

-   Also, as the sample size increases the shape of the sampling distribution becomes more similar to a normal distribution regardless of the shape of the population.

The example "NFL Contracts (2015 in millions)" was used to construct the two sampling distributions below.

In the first, a sample size of 10 was used. In the second, a sample size of 100 was used.

```{r}
n=10 # sample size 
# Building the results matrix
s<-matrix(NA, nrow=10, ncol=1000) 
for (i in 1:1000){
  s[,i]=nfl$YearlySalary[sample(1:2099,n)]
  }
mean.s1=colMeans(s) # respective 1000 sample means
hist(mean.s1, 
     main="Histogram of 1000 sample means", 
     xlab=substitute(paste("Mean=", 
                           mean, "; Stdev=",
                           std, " of 1000 samples; n=10"),
                     list(mean=signif(mean(mean.s1),3),
                          std=signif(sd(mean.s1),3))))

```

For a sample size of 100

```{r}
n=100 # sample size 
s<-matrix(NA, nrow=100, ncol=1000)
for (i in 1:1000){
  s[,i]=nfl$YearlySalary[sample(1:2099,n)]
  }
mean.s2=colMeans(s) # respective 1000 sample means
hist(mean.s2, 
     main="Histogram of 1000 sample means",
     xlab=substitute(paste("Mean=", 
                           mean, "; Stdev=",
                           std, " of 1000 samples; n=100"),
                     list(mean=signif(mean(mean.s2),3),
                          std=signif(sd(mean.s2),3))))

```

-   With a sample size of 10, the standard error of the mean was 0.98.

-   With a sample size of 100 the standard error of the mean was 0.3.

-   When the sample size increased the standard error decreased.

-   Also note that the population was strongly skewed to the right.

-   With the smaller sample size, the sampling distribution was also skewed to the right, though not as strongly skewed as the population.

-   With the larger sample size, the sampling distribution was approximately normal.

## Introduction to Confidence intervals

In real life, we don't typically have access to the whole population.

In these cases we can use the sample data that we do have to construct a confidence interval to estimate the population parameter with a stated level of confidence.

This is one type of statistical inference.

**A Confidence Interval** is a range computed using sample statistics to estimate an unknown population parameter with a stated level of confidence.

### Point Estimate

Sample statistic that serves as the best estimate for a population parameter.

### Margin of Error

Half of the width of a confidence interval; equal to the multiplier times the standard error.

#### General Form of Confidence Interval

-   Sample statistic ± margin of error

-   Margin of error = multiplier × standard error

The confidence level is set by the researcher. As for "large" samples sample statistics follow a symmetric distribution we can use normal distribution to set its values:

-   From the Empirical rule -\> In a normal distribution 95 % of observations fall within 2 Std. deviations of the mean.

Thus,

-   For a 95% Confidence interval we ca assume a multiplier of 2 (\* Std. Error) -\> sample statistic ± 2 (standard error)

#### Interpretation of a Confidence level

"We are 95% confident that the population parameter is between X and Y .

## Bootstrapping

In order to construct a confidence interval we need information about the sampling distribution.

If the actual population parameter are not known, we can use bootstrapping methods to construct a bootstrap sampling distribution to construct a confidence interval.

Bootstrapping is a resampling procedure that uses data from one sample to generate a sampling distribution by repeatedly taking random samples from the known sample.

### Bootstrapping procedure example

We have data concerning the heights of individuals in a random sample of n = 15.

To construct a bootstrap distribution for the mean height we would:

1.  Randomly select one individual from that sample and record their height.
2.  with that individual placed back into the sample, we would randomly select a second individual and record their height. -\> sampling with replacement.
3.  Repeat this process until we have selected 15 values.
4.  Use those 15 selected values to compute a bootstrapped sample mean.
5.  Repeat many times.

The distribution of many bootstrapped sample means is known as the **bootstrap distribution** or **bootstrap sampling distribution.**

```{r}
s=c(1,1,0,1,1,0,1,0,0,0,1,1,0,1,0,1,1,0,0,1,0,0,0,1,1,0,1,0,0,1,0,0,1,0,1,0,1,1,1,1,0,1,1,1,1,1,0,1,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,1,1,1,0,0,1,1,0,0,1,1,1,0,1,0,1,1,0,1,1,1,0,1,0,1,0,1,1,1,0,0,1,0,0,1,0,1)

# how many 1 do we have?
count=sum(s)
count

# how many values do we have?
n=length(s)
n

# the population proportion parameter
p=count/n 
p

# Applying bootstrapping , sample size of 100: 
  S.boot=matrix(0, nrow=100, ncol=5000) 
  # to store the data
  S.boot[,1]=s[sample(1:100,100,replace=TRUE)] 
  # first sample
  sum(S.boot[,1]) # count
  sum(S.boot[,1])/100 # proportion

  # additional 999 samples 
  for (i in 2:1000){
    S.boot[,i]=s[sample(1:100,100,replace=TRUE)]
  }
  # 
  mean.s=colMeans(S.boot[,1:1000]) # 1000 sample means
  # ploting the histogram of the 1000 sample means
  plot(table(mean.s), 
       main="Histogram of 1000 sample means",
       ylab="Frequency",
       xlab=substitute(paste("Mean=", 
                             mean, 
                             "; Stdev=",
                             std, " of 1000 samples; n=100"),
                       list(mean=signif(mean(mean.s),3),
                            std=signif(sd(mean.s),3))))


```

```{r}

# The same but for 5000 samples
for (i in 1001:5000){
  S.boot[,i]=s[sample(1:100,100,replace=TRUE)]
  } 
#
mean.s=colMeans(S.boot)
plot(table(mean.s), 
     main="Histogram of 5000 sample means",
     ylab="Frequency", 
     xlab=substitute(paste("Mean=", 
                           mean, "; Stdev=",
                           std, " of 5000 samples; n=100"),
                     list(mean=signif(mean(mean.s),3), 
                          std=signif(sd(mean.s),3))))

```

### Bootstrap confidence interval

Once we have a bootstrap sampling distribution there are two methods for constructing a confidence interval:

1.  **Standard Error - For normal Sampling distributions:**

    -   The standard deviation of the bootstrap distribution is the standard error.

    -   For a 95% confidence interval = +/- 2\* Std. Error

2.  **Percentile Method**

    -   Find the middle 95% of bootstrap statistics

        (Preferred as it does not depend on the sampling distribution shape)

E.g Using the Standard deviation Error

-   Determine what type of variable(s) you have and what parameters you want to estimate.

-   Using the sample data, generate at least 5,000 bootstrap samples.

-   Confirm that your bootstrap distribution is approximately normal. If it's not approximately normal you should consider using the percentile method.

-   Use your original sample statistic and the standard error from your bootstrap distribution to construct a confidence interval. For a 95% confidence interval the formula is sample statistics ± 2(standard error).

##### Example in R

In a representative sample of 500 Portuguese adults, 78 are lactose intolerant. Use R to construct a 95% confidence interval to estimate the proportion of all Portuguese adults who are lactose intolerant

```{r}
s=c(0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,1,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,0,1,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,1,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0)

# our sample size
n=length(s) 
n

# our sample proportion
p=sum(s)/n
p

# let's bootstrap
# matrix object to store the data
S.boot=matrix(0, nrow=500, ncol=5000) # to store the data 
# the bootstrap itself
for (i in 1:5000){
  S.boot[,i]=s[sample(1:500,500,replace=TRUE)]
  } # 5000 samples 

#our bootstrap statistics
mean.s=colMeans(S.boot)# 5000 sample means
# just a glimpse of it
head(mean.s)
round(sd(mean.s),4) # standard error of the mean
# ploting our sampling distribution
plot(table(mean.s), 
     main="Histogram of 5000 sample means", 
     xlab=substitute(paste("Mean=", 
                           mean, "; Stdev=",
                           std, " of 5000 samples; n=500"),
                     list(mean=round(mean(mean.s),3),
                          std=signif(sd(mean.s,),3))),
     ylab="Frequency",
     sub=substitute(paste("Confidence interval: statistic +/- 2(standard error)=[", 
                          LL,";",
                          UL,
                          "]"), 
                    list(LL=round(mean.s-2*sd(mean.s),3),
                         UL=round(mean.s+2*sd(mean.s),3))))

```

##### Example 2

In a random sample of adults, 9 out of the 20 females were dieting and 4 out of 15 males were dieting. Construct a 90% confidence interval to estimate de difference in the proportion of females and males in the population who are dieting.

```{r}

sF=c(0,1,0,1,0,1,1,0,0,0,1,0,0,1,1,0,1,0,1,0)
sM=c(1,0,0,1,1,0,0,1,0,0,0,0,0,0,0) 

nF=length(sF) 
nF

nM=length(sM) 
nM

pF=sum(sF)/nF
pF

pM=sum(sM)/nM 
pM

S.bootF=matrix(0, nrow=20, ncol=5000) # to store the data
S.bootM=matrix(0, nrow=15, ncol=5000) # to store the data 
# 
for (i in 1:5000){
  S.bootF[,i]=sF[sample(1:20,20,replace=TRUE)]
  } # 5000 samples for females

for (i in 1:5000){
  S.bootM[,i]=sM[sample(1:15,15,replace=TRUE)]
  } # 5000 samples for males


mean.sF=colMeans(S.bootF) # 5000 sample means
mean.sM=colMeans(S.bootM) # 5000 sample means
df=mean.sF-mean.sM # differences of 5000 samples
hist(df, 
     main="Histogram of 5000 differences of sample proportions", 
     xlab=substitute(paste("Mean=", 
                           mean, 
                           "; Stdev=",
                           std,
                           " of 5000 samples; n=500"),
                     list(mean=round(mean(df),3), 
                          std=sd(df,4))),
     sub=substitute(paste("Confidence interval: [", Perc[5],";",Perc[95],"]=[", LL,";",UL,"]"),
                    list(LL=signif(quantile(df,0.05),3),
                         UL=signif(quantile(df,0.95),3))))

```

### Paired Samples

When we have two independent samples, the observations in the two groups are unrelated to one another and are not matched in any meaningful way.

With paired samples, the observations in the two groups are matched in a meaningful way.

Most often this occurs when data are collected twice from the same participants, such as in a pre-test / post-test design.

When data are paired, we compute the difference for each case, and then treat those differences as if they are a single measure.

When constructing a confidence interval for the difference in paired means, we're really constructing a confidence interval for a single mean, where the single mean is the mean difference.

-   The population parameter is µd where µd = µ1 − µ2.

-   The sample statistic is ¯xd where ¯xd = ¯x1 − ¯x2.

##### Example in R

```{r}
# missing the dataset to use on this example
```

### Effect of sample size on confidence intervals

-   As the sample size increases the standard error decreases.

-   With a larger sample size there is less variation between sample statistics, or in this case bootstrap statistics.

-   When the sample size increases the confidence interval becomes more narrow.
